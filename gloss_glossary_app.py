import os
import hmac
import streamlit as st

def check_password():
    if "password_ok" not in st.session_state:
        st.session_state["password_ok"] = False

    def on_enter():
        if hmac.compare_digest(st.session_state.get("pw", ""), os.environ.get("APP_PASSWORD", "")):
            st.session_state["password_ok"] = True
            st.session_state["pw"] = ""
        else:
            st.session_state["password_ok"] = False

    if st.session_state["password_ok"]:
        return True

    st.text_input("Password", type="password", key="pw", on_change=on_enter)
    if "pw" in st.session_state and st.session_state["pw"] and not st.session_state["password_ok"]:
        st.error("Password incorrect")
    return False

if not check_password():
    st.stop()

import re
import streamlit as st
import pandas as pd

# ---------------------------------------------------
# 1) åŸºæœ¬ç•¥å·è¾æ›¸ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¢—ã‚„ã™ï¼‰
# ---------------------------------------------------
DEFAULT_GLOSSARY = {
    "ACC": "accusative",
    "DAT": "dative",
    "GEN": "genitive",
    "ABL": "ablative",
    "LOC": "locative",
    "INS": "instrumental",
    "PL": "plural",
    "SG": "singular",
    "POSS": "possessive",
    "IMP": "imperative",
    "PROG": "progressive",
    "Q": "question particle",
    "VN": "verbal noun",

    # person/number shorthand
    "1": "1st person",
    "2": "2nd person",
    "3": "3rd person",
    "1SG": "1st person singular",
    "2SG": "2nd person singular",
    "3SG": "3rd person singular",
    "1PL": "1st person plural",
    "2PL": "2nd person plural",
    "3PL": "3rd person plural",

    # common compounded forms
    "1SG.POSS": "1st person singular possessive",
    "2SG.POSS": "2nd person singular possessive",
    "3SG.POSS": "3rd person singular possessive",
    "1PL.POSS": "1st person plural possessive",
    "2PL.POSS": "2nd person plural possessive",
    "3PL.POSS": "3rd person plural possessive",

    # verbal morphology
    "PTCP": "participle",
    "PTCP.PAST": "past participle",
    "PTCP.NPST": "non-past participle",
    "CVB": "converb",
    "CVB.SEQ": "sequential converb",
    "CVB.CNT": "continuative converb",
}

# ---------------------------------------------------
# 2) ã‚°ãƒ­ã‚¹è¡ŒæŠ½å‡ºï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰
# ---------------------------------------------------
def extract_gloss_lines(text, min_hyphen_tokens=2):
    """
    ã€Œã‚°ãƒ­ã‚¹è¡Œã£ã½ã„è¡Œã€ã ã‘ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    ãƒ«ãƒ¼ãƒ«ï¼šãƒã‚¤ãƒ•ãƒ³ä»˜ããƒˆãƒ¼ã‚¯ãƒ³ãŒä¸€å®šæ•°ä»¥ä¸Šå«ã¾ã‚Œã‚‹è¡Œã‚’ã‚°ãƒ­ã‚¹è¡Œã¨ã¿ãªã™ã€‚
    """
    gloss_lines = []
    for line in text.splitlines():
        line_strip = line.strip()
        if not line_strip:
            continue

        # ãƒã‚¤ãƒ•ãƒ³ã‚’å«ã‚€ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        tokens = re.split(r"\s+", line_strip)
        hyphen_tokens = [t for t in tokens if "-" in t]

        # ä¾‹: coachman-PL grain-ACC bazaar-DAT ...
        if len(hyphen_tokens) >= min_hyphen_tokens:
            gloss_lines.append(line_strip)

    return gloss_lines


# ---------------------------------------------------
# 3) ç•¥å·æŠ½å‡º
# ---------------------------------------------------
ABBR_PATTERN = re.compile(r"^(?:[0-9]*[A-Z]+(?:\.[A-Z0-9]+)*)$")

def extract_abbreviations_from_gloss_lines(gloss_lines):
    """
    ã‚°ãƒ­ã‚¹è¡Œã‹ã‚‰ç•¥å·ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    ãƒ»å˜èªã‚’ãƒã‚¤ãƒ•ãƒ³ã‚„ã‚¤ã‚³ãƒ¼ãƒ«ã§å‰²ã‚Šã€å¾ŒåŠå´ã‚’å€™è£œã«ã™ã‚‹
    ãƒ»ã•ã‚‰ã« '.' åŒºåˆ‡ã‚Šã§è¤‡åˆç•¥å·ã‚‚æ‹¾ã†
    """
    abbreviations = []

    for line in gloss_lines:
        # è¨˜å·ã§åˆ†å‰²ï¼ˆç©ºç™½ã€ã‚¿ãƒ–ï¼‰
        tokens = re.split(r"\s+", line)

        for token in tokens:
            # ã¾ãš "=" ã‚’åˆ†å‰²
            parts_eq = token.split("=")
            for peq in parts_eq:
                # "-" ã§åˆ†å‰²ï¼ˆæœ€åˆã¯èªå¹¹ãªã®ã§å¾Œã‚å´ã‚’ä¸»ã«è¦‹ã‚‹ï¼‰
                parts_hy = peq.split("-")

                # 2PL.POSS-GEN ã¿ãŸã„ãªå ´åˆ
                for ph in parts_hy[1:]:  # å¾Œã‚å´ã ã‘
                    ph = ph.strip(".,;:()[]{}\"'")

                    # ã•ã‚‰ã« "." ã§è¤‡åˆç•¥å·ã‚’æ‹¾ã†ï¼ˆPTCP.PASTãªã©ï¼‰
                    # "2PL.POSS" ã¯ãã®ã¾ã¾1ã¤ã¨ã—ã¦ã‚‚æ¡ã‚‹
                    if ABBR_PATTERN.match(ph):
                        abbreviations.append(ph)

                    # ã‚‚ã— "2PL.POSS" å†…ã®è¦ç´ ã‚‚æ¬²ã—ã‘ã‚Œã°åˆ†å‰²ã—ã¦æ‹¾ã†:
                    # ä¾‹: 2PL.POSS â†’ 2PL, POSS ã‚‚æ‹¾ã†
                    if "." in ph:
                        subparts = ph.split(".")
                        for sp in subparts:
                            if ABBR_PATTERN.match(sp):
                                abbreviations.append(sp)

    return abbreviations


def build_glossary_table(abbreviations, glossary_dict):
    """
    ç•¥å·ãƒªã‚¹ãƒˆã‹ã‚‰ (Abbreviation, Meaning, Count) ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œã‚‹
    """
    freq = {}
    for abbr in abbreviations:
        freq[abbr] = freq.get(abbr, 0) + 1

    rows = []
    for abbr, count in sorted(freq.items(), key=lambda x: (-x[1], x[0])):
        meaning = glossary_dict.get(abbr, "")
        rows.append({"Abbreviation": abbr, "Meaning": meaning, "Count": count})

    return pd.DataFrame(rows)


# ---------------------------------------------------
# Streamlit UI
# ---------------------------------------------------
st.set_page_config(page_title="Glossary Generator", layout="wide")
st.title("ğŸ“Œ ã‚°ãƒ­ã‚¹ç•¥å·è¾æ›¸ï¼ˆAbbreviation Glossaryï¼‰è‡ªå‹•ç”Ÿæˆã‚¢ãƒ—ãƒª")

st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã¯ä»¥ä¸‹ã‚’è‡ªå‹•ã§è¡Œã„ã¾ã™ï¼š

âœ… **ã‚°ãƒ­ã‚¹è¡Œã ã‘ã‚’æŠ½å‡º**ï¼ˆè¨³æ–‡ãƒ»å‚è€ƒæ–‡çŒ®ãªã©ã®ãƒã‚¤ã‚ºã‚’é™¤å¤–ï¼‰  
âœ… **ç•¥å·ã‚’æŠ½å‡º**ï¼ˆACC, PL ã ã‘ã§ãªã **1PL, 3SG, 2PL.POSS, PTCP.PAST** ãªã©ã‚‚æ‹¾ã†ï¼‰  
âœ… **ç•¥å·â†’æ„å‘³ã‚’è‡ªå‹•è£œå®Œ**ï¼ˆè¾æ›¸ã«ã‚ã‚‹ã‚‚ã®ã¯MeaningãŒè‡ªå‹•å…¥åŠ›ï¼‰  
âœ… **è¡¨ã‚’ç·¨é›†ã—ã¦CSVã§å‡ºåŠ›**
""")

example_text = """(1) aravakaÅ¡-lar Ä¡ala-ni bozor-Ä¡a al-Ã¯b bor-a
coachman-PL grain-ACC bazaar-DAT take-CVB.SEQ go-CVB.CNT
yat-Ã¯b=dur.
lie-PROG=3SG
ã€Œå¾¡è€…ã¯ç©€ç‰©ã‚’ãƒã‚¶ãƒ¼ãƒ«ã«æŒã£ã¦è¡Œã£ã¦ã„ã‚‹ã¨ã“ã‚ã ã€‚ã€
"""

text_input = st.text_area("ğŸ“¥ ã“ã“ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„", value=example_text, height=260)

col1, col2, col3 = st.columns([1, 1, 2])

with col1:
    min_hyphen_tokens = st.number_input("ã‚°ãƒ­ã‚¹è¡Œåˆ¤å®šï¼šãƒã‚¤ãƒ•ãƒ³èªæ•°", min_value=1, max_value=10, value=2)

with col2:
    run_button = st.button("ğŸ” Glossaryç”Ÿæˆ")

if run_button:
    gloss_lines = extract_gloss_lines(text_input, min_hyphen_tokens=min_hyphen_tokens)

    st.subheader("âœ… æŠ½å‡ºã•ã‚ŒãŸã‚°ãƒ­ã‚¹è¡Œ")
    if gloss_lines:
        st.code("\n".join(gloss_lines))
    else:
        st.warning("ã‚°ãƒ­ã‚¹è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒã‚¤ãƒ•ãƒ³èªæ•°ã®é–¾å€¤ã‚’ä¸‹ã’ã‚‹ã¨æ”¹å–„ã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")
        st.stop()

    abbreviations = extract_abbreviations_from_gloss_lines(gloss_lines)
    if not abbreviations:
        st.warning("ç•¥å·ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    df = build_glossary_table(abbreviations, DEFAULT_GLOSSARY)

    st.subheader("âœ… ç•¥å·ä¸€è¦§ï¼ˆMeaningã¯ç·¨é›†å¯èƒ½ï¼‰")
    edited_df = st.data_editor(df, use_container_width=True, num_rows="dynamic")

    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    csv = edited_df.to_csv(index=False).encode("utf-8")
    st.download_button(
        label="â¬‡ï¸ CSVã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="abbreviation_glossary.csv",
        mime="text/csv"
    )
