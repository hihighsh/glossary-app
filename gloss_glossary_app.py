import os
import hmac
import re
import streamlit as st
import pandas as pd

# =========================
# 0) Password Gate (Streamlit Cloud Secrets: APP_PASSWORD="...")
# =========================
def check_password():
    if "password_ok" not in st.session_state:
        st.session_state["password_ok"] = False

    def on_enter():
        if hmac.compare_digest(st.session_state.get("pw", ""), os.environ.get("APP_PASSWORD", "")):
            st.session_state["password_ok"] = True
            st.session_state["pw"] = ""
        else:
            st.session_state["password_ok"] = False

    if st.session_state["password_ok"]:
        return True

    st.text_input("Password", type="password", key="pw", on_change=on_enter)
    if "pw" in st.session_state and st.session_state["pw"] and not st.session_state["password_ok"]:
        st.error("Password incorrect")
    return False

if not check_password():
    st.stop()


# =========================
# 1) Base Glossary
# =========================
BASE_GLOSSARY = {
    "1": "1st person",
    "2": "2nd person",
    "3": "3rd person",

    "SG": "singular",
    "PL": "plural",
    "POSS": "possessive",

    "ACC": "accusative",
    "DAT": "dative",
    "GEN": "genitive",
    "ABL": "ablative",
    "LOC": "locative",
    "INS": "instrumental",

    "VN": "verbal noun",
    "IMP": "imperative",
    "PROG": "progressive",
    "Q": "question particle",

    "PTCP": "participle",
    "PAST": "past",
    "NPST": "non-past",
    "CVB": "converb",
    "SEQ": "sequential",
    "CNT": "continuative",

    "PTCP.PAST": "past participle",
    "PTCP.NPST": "non-past participle",
    "CVB.SEQ": "sequential converb",
    "CVB.CNT": "continuative converb",

    "1SG": "1st person singular",
    "2SG": "2nd person singular",
    "3SG": "3rd person singular",
    "1PL": "1st person plural",
    "2PL": "2nd person plural",
    "3PL": "3rd person plural",
}

# category sets (used as fallback when CSV doesn't provide Category)
PERSON_SET = {"1", "2", "3"}
NUMBER_SET = {"SG", "PL"}
CASE_SET = {"ACC", "DAT", "GEN", "ABL", "LOC", "INS"}
POSSESSION_SET = {"POSS"}
VERB_MORPH_SET = {"PTCP", "CVB", "VN", "IMP"}
TAM_ASPECT_SET = {"PAST", "NPST", "PROG"}
MISC_SET = {"Q", "SEQ", "CNT"}

def categorize_abbr(abbr: str) -> str:
    if abbr in PERSON_SET:
        return "person"
    if abbr in NUMBER_SET:
        return "number"
    if abbr in CASE_SET:
        return "case"
    if abbr in POSSESSION_SET:
        return "possession"
    if abbr in VERB_MORPH_SET:
        return "verbal morphology"
    if abbr in TAM_ASPECT_SET:
        return "tense/aspect/mood"
    if abbr in MISC_SET:
        return "other"
    if re.fullmatch(r"[0-9]+", abbr):
        return "person"
    if re.fullmatch(r"[0-9]+[A-Z]+", abbr):
        return "agreement"
    if "." in abbr:
        return "compound"
    return ""


# =========================
# 2) Load glossary CSV (optional)
# =========================
def load_glossary_csv(uploaded_file) -> tuple[dict, dict]:
    """
    Returns:
      (abbr_to_meaning, abbr_to_category)
    Accepts CSVs with at least Abbreviation, Meaning.
    Category is optional.
    """
    df = pd.read_csv(uploaded_file)
    cols = {c.lower(): c for c in df.columns}

    if "abbreviation" not in cols:
        raise ValueError("CSVã« 'Abbreviation' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    if "meaning" not in cols:
        raise ValueError("CSVã« 'Meaning' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    abbr_col = cols["abbreviation"]
    meaning_col = cols["meaning"]
    cat_col = cols.get("category")

    abbr_to_meaning = {}
    abbr_to_category = {}

    for _, row in df.iterrows():
        abbr = str(row.get(abbr_col, "")).strip()
        if not abbr:
            continue
        meaning = str(row.get(meaning_col, "")).strip()
        if meaning and meaning.lower() != "nan":
            abbr_to_meaning[abbr] = meaning
        if cat_col:
            cat = str(row.get(cat_col, "")).strip()
            if cat and cat.lower() != "nan":
                abbr_to_category[abbr] = cat

    return abbr_to_meaning, abbr_to_category


# =========================
# 3) Gloss line extraction (noise reduction) '-' OR '='
# =========================
def extract_gloss_lines(text: str, min_marked_tokens: int = 2) -> list[str]:
    gloss_lines = []
    for line in text.splitlines():
        s = line.strip()
        if not s:
            continue
        tokens = re.split(r"\s+", s)
        marked_tokens = [t for t in tokens if ("-" in t or "=" in t)]
        if len(marked_tokens) >= min_marked_tokens:
            gloss_lines.append(s)
    return gloss_lines


# =========================
# 4) Abbreviation extraction + decomposition (handles '-' and '=')
# =========================
ABBR_PATTERN = re.compile(r"^(?:[0-9]*[A-Z]+(?:\.[A-Z0-9]+)*)$")
NUM_ALPHA_PATTERN = re.compile(r"^([0-9]+)([A-Z]+)$")  # 3SG -> 3 + SG

def _add_decomposed_units(abbr: str, out: list[str]) -> None:
    dot_parts = abbr.split(".") if "." in abbr else [abbr]
    for part in dot_parts:
        part = part.strip()
        if not part:
            continue
        if ABBR_PATTERN.match(part):
            out.append(part)
        m = NUM_ALPHA_PATTERN.match(part)
        if m:
            out.append(m.group(1))  # 3
            out.append(m.group(2))  # SG

def extract_abbreviations_from_gloss_lines(gloss_lines: list[str], enable_decomp: bool) -> list[str]:
    abbreviations: list[str] = []

    for line in gloss_lines:
        tokens = re.split(r"\s+", line)
        for token in tokens:
            eq_parts = token.split("=")
            for part_eq in eq_parts:
                part_eq = part_eq.strip(".,;:()[]{}\"'")

                # whole segment itself (e.g., "3SG")
                if ABBR_PATTERN.match(part_eq):
                    abbreviations.append(part_eq)
                    if enable_decomp:
                        _add_decomposed_units(part_eq, abbreviations)

                # hyphen suffixes (e.g., lie-PROG -> PROG)
                parts_hy = part_eq.split("-")
                for suf in parts_hy[1:]:
                    suf = suf.strip(".,;:()[]{}\"'")
                    if not ABBR_PATTERN.match(suf):
                        continue
                    abbreviations.append(suf)
                    if enable_decomp:
                        _add_decomposed_units(suf, abbreviations)

    return abbreviations


# =========================
# 5) Table builder (uses merged glossary + optional category overrides)
# =========================
def build_glossary_table(abbreviations: list[str], abbr_to_meaning: dict, abbr_to_category: dict) -> pd.DataFrame:
    freq: dict[str, int] = {}
    for abbr in abbreviations:
        freq[abbr] = freq.get(abbr, 0) + 1

    rows = []
    for abbr, count in sorted(freq.items(), key=lambda x: (-x[1], x[0])):
        meaning = abbr_to_meaning.get(abbr, "")
        category = abbr_to_category.get(abbr, "") or categorize_abbr(abbr)
        rows.append({
            "Category": category,
            "Abbreviation": abbr,
            "Meaning": meaning,
            "Count": count,
        })

    return pd.DataFrame(rows)[["Category", "Abbreviation", "Meaning", "Count"]]


# =========================
# 6) Streamlit UI
# =========================
st.set_page_config(page_title="Glossary Generator", layout="wide")
st.title("ğŸ“Œ ã‚°ãƒ­ã‚¹ç•¥å·è¾æ›¸ï¼ˆAbbreviation Glossaryï¼‰ç”Ÿæˆ")

# ---- Sidebar: glossary CSV upload
st.sidebar.header("ğŸ“š ç•¥å·è¾æ›¸CSVã®å–ã‚Šè¾¼ã¿ï¼ˆä»»æ„ï¼‰")
uploaded = st.sidebar.file_uploader("CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["csv"])

use_uploaded_glossary = st.sidebar.checkbox("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸè¾æ›¸ã‚’ä½¿ç”¨", value=True)
prefer_uploaded = st.sidebar.checkbox("åŒã˜ç•¥å·ãŒã‚ã‚‹å ´åˆã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¾æ›¸ã‚’å„ªå…ˆ", value=True)

# Load uploaded glossary (if provided)
uploaded_meaning = {}
uploaded_category = {}
upload_error = None

if uploaded is not None and use_uploaded_glossary:
    try:
        uploaded_meaning, uploaded_category = load_glossary_csv(uploaded)
        st.sidebar.success(f"èª­ã¿è¾¼ã¿OKï¼š{len(uploaded_meaning)}ä»¶ï¼ˆMeaningï¼‰ / {len(uploaded_category)}ä»¶ï¼ˆCategoryï¼‰")
    except Exception as e:
        upload_error = str(e)
        st.sidebar.error(upload_error)

# Merge glossary meanings
if prefer_uploaded:
    # uploaded overwrites base
    MERGED_MEANING = {**BASE_GLOSSARY, **uploaded_meaning}
else:
    # base overwrites uploaded
    MERGED_MEANING = {**uploaded_meaning, **BASE_GLOSSARY}

# Categories: uploaded categories override if provided
MERGED_CATEGORY = dict(uploaded_category)  # only explicit overrides live here


# ---- Main input and controls
if "input_text" not in st.session_state:
    st.session_state["input_text"] = """(1) aravakaÅ¡-lar Ä¡ala-ni bozor-Ä¡a al-Ã¯b bor-a
coachman-PL grain-ACC bazaar-DAT take-CVB.SEQ go-CVB.CNT
yat-Ã¯b=dur.
lie-PROG=3SG
ã€Œå¾¡è€…ã¯ç©€ç‰©ã‚’ãƒã‚¶ãƒ¼ãƒ«ã«æŒã£ã¦è¡Œã£ã¦ã„ã‚‹ã¨ã“ã‚ã ã€‚ã€
"""

top_left, top_right = st.columns([1, 2])
with top_left:
    if st.button("ğŸ§¹ Clearï¼ˆå…¥åŠ›ã‚’å³æ¶ˆå»ï¼‰"):
        st.session_state["input_text"] = ""
        st.rerun()

with top_right:
    show_gloss_lines = st.checkbox("æŠ½å‡ºã•ã‚ŒãŸã‚°ãƒ­ã‚¹è¡Œã‚’è¡¨ç¤ºï¼ˆäº‹æ•…é˜²æ­¢ã®ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆOFFï¼‰", value=False)

text_input = st.text_area(
    "ğŸ“¥ ã“ã“ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„",
    key="input_text",
    height=260
)

col1, col2, col3 = st.columns([1, 1.4, 2])
with col1:
    min_marked_tokens = st.number_input("ã‚°ãƒ­ã‚¹è¡Œåˆ¤å®šï¼š '-' ã¾ãŸã¯ '=' ã‚’å«ã‚€èªæ•°", min_value=1, max_value=10, value=2)
with col2:
    enable_decomp = st.checkbox("ç•¥å·ã‚’åˆ†è§£ã—ã¦å€‹åˆ¥ç•¥å·ã‚‚ä¸€è¦§åŒ–ï¼ˆPTCP.PASTâ†’PTCP+PAST / 3SGâ†’3+SGï¼‰", value=True)
with col3:
    run_button = st.button("ğŸ” Glossaryç”Ÿæˆ", use_container_width=True)

if run_button:
    if upload_error:
        st.warning("è¾æ›¸CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¦ã„ã‚‹ãŸã‚ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¾æ›¸ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚")

    gloss_lines = extract_gloss_lines(text_input, min_marked_tokens=int(min_marked_tokens))
    if not gloss_lines:
        st.warning("ã‚°ãƒ­ã‚¹è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚é–¾å€¤ï¼ˆèªæ•°ï¼‰ã‚’ä¸‹ã’ã‚‹ã¨æ”¹å–„ã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")
        st.stop()

    if show_gloss_lines:
        st.subheader("âœ… æŠ½å‡ºã•ã‚ŒãŸã‚°ãƒ­ã‚¹è¡Œ")
        st.code("\n".join(gloss_lines))

    abbreviations = extract_abbreviations_from_gloss_lines(gloss_lines, enable_decomp=enable_decomp)
    if not abbreviations:
        st.warning("ç•¥å·ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    df = build_glossary_table(abbreviations, MERGED_MEANING, MERGED_CATEGORY)

    st.subheader("âœ… ç•¥å·ä¸€è¦§ï¼ˆCategory/Meaning ã¯ç·¨é›†å¯èƒ½ï¼‰")
    edited_df = st.data_editor(df, use_container_width=True, num_rows="dynamic")

    csv = edited_df.to_csv(index=False).encode("utf-8")
    st.download_button(
        label="â¬‡ï¸ CSVã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="abbreviation_glossary.csv",
        mime="text/csv"
    )
